{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *grama* Demo\n",
    "\n",
    "---\n",
    "\n",
    "*grama* is a *grammar of model analysis*---a language for describing and analyzing mathematical models. Heavily inspired by [ggplot](https://ggplot2.tidyverse.org/index.html), `py_grama` is a Python package that implements *grama* by providing tools for defining and exploring models. This notebook illustrates how one can use *grama*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup\n",
    "from dfply import *\n",
    "import grama as gr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Tour: Analyzing a model\n",
    "\n",
    "---\n",
    "\n",
    "*grama* separates the model *definition* from model *analysis*; once the model is fully defined, only minimal information is necessary for further analysis.\n",
    "\n",
    "As a quick demonstration, we import a fully-defined model provided with *grama*, and carry out a few analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grama.models import make_cantilever_beam\n",
    "\n",
    "model_beam = make_cantilever_beam()\n",
    "model_beam.printpretty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `printpretty()` gives us a quick summary of the model; we can see this model has two deterministic variables `w,t` and four random variables `H,V,E,Y`, all of which affect the outputs `c_area, g_stress, g_displacement`. Since there are random variables, there is a source of *uncertainty* which we must consider when studying this model.\n",
    "\n",
    "## Studying model behavior with uncertainty\n",
    "\n",
    "Since the model has sources of randomness (`var_rand`), we must account for this when studying its behavior. We can do so through a Monte Carlo analysis. We make decisions about the deterministic inputs by specifying `df_det`, and the `py_grama` function `gr.ev_monte_carlo` automatically handles the random inputs. Below we fix a nominal value `w = 0.5 * (2 + 4)`, sweep over values for `t`, and account for the randomness via Monte Carlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate data for deterministic variables\n",
    "df_beam_det = pd.DataFrame(\n",
    "    data={\n",
    "        \"w\": [0.5 * (2 + 4)] * 10,\n",
    "        \"t\": np.linspace(2.5, 3, num=10)\n",
    "    }\n",
    ")\n",
    "\n",
    "## Carry out a Monte Carlo analysis of the random variables\n",
    "df_beam_mc = \\\n",
    "    model_beam >> \\\n",
    "    gr.ev_monte_carlo(n=1e2, df_det=df_beam_det)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help plot the data, we use `dfply` to wrange the data, and `seaborn` to quickly visualize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beam_wrangled = \\\n",
    "    df_beam_mc >> \\\n",
    "    gather(\"output\", \"y\", [\"c_area\", \"g_stress\", \"g_displacement\"])\n",
    "\n",
    "g = sns.FacetGrid(df_beam_wrangled, col=\"output\", sharey=False)\n",
    "g.map(sns.lineplot, \"t\", \"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean behavior of the model is shown as a solid line, while the band visualizes the standard deviation of the model output. From this plot, we can see:\n",
    "\n",
    "- The random variables have no effect on `c_area` (there is no band)\n",
    "- Comparing `g_stress` and `g_displacement`, the former is more strongly affected by the random inputs, as illustrated by its wider uncertainty band.\n",
    "\n",
    "While this provides a visual description of how uncertainty affects our outputs, we might be interested in *how* the different random variables affect our outputs.\n",
    "\n",
    "## Probing random variable effects\n",
    "\n",
    "One way to quantify the effects of random variables is through *Sobol' indices*, which quantify variable importance by the fraction of output variance \"explained\" by each random variable. Since distribution information is included in the model, we can carry out a *hybrid-point Monte Carlo* and analyze the results with two calls to `py_grama`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sobol = \\\n",
    "    model_beam >> \\\n",
    "    gr.ev_hybrid(n_samples=1e3, df_det=\"nom\", seed=101) >> \\\n",
    "    gr.tf_sobol()\n",
    "\n",
    "df_sobol >> \\\n",
    "    select(X.g_stress, X.g_displacement, X.ind) >> \\\n",
    "    mask(str_detect(X.ind, \"S_\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results suggest that `g_stress` is largely insensitive to `E`, while `g_displacement` is insensitive to `Y`. For `g_displacement`, the input `V` contributes about three times the variance as variables `H,E`.\n",
    "\n",
    "To get a *qualitative* sense of how the random variables affect our model, we can perform a set of sweeps over random variable space with a *sinew* design. We use `py_grama` to generate the design, then use `dfply` to wrangle the data for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beam_sweeps = \\\n",
    "    model_beam >> \\\n",
    "    gr.ev_sinews(n_density=50, n_sweeps=10, df_det=\"nom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we visualize the design in the four-dimensional random variable space of `[H,V,E,Y]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    data=df_beam_sweeps,\n",
    "    vars=model_beam.var_rand,\n",
    "    hue=\"sweep_ind\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the sweeps cross the domain in straight lines at random starting locations. Each of these sweeps gives us a \"straight shot\" within a single variable. Visualizing the outputs for these sweeps will give us a sense of a single variable's influence, contextualized by the effects of the other random variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    data=df_beam_sweeps >> \\\n",
    "        gather(\"input\", \"x\", model_beam.var_rand) >> \\\n",
    "        gather(\"output\", \"y\", model_beam.outputs) >> \\\n",
    "        mask(X.sweep_var == X.input),\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    hue=\"sweep_ind\",\n",
    "    col=\"input\",\n",
    "    row=\"output\",\n",
    "    kind=\"line\",\n",
    "    facet_kws=dict(sharex=False, sharey=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this plot, we can see:\n",
    "\n",
    "- The output `c_area` is insensitive to all the random variables\n",
    "- As the Sobol' analyis above suggested `g_stress` is insensitive to `E`, and `g_displacement` is insensitive to `Y`\n",
    "- Visualizing the results shows that inputs `H,E` tend to 'saturate' in their effects on `g_displacement`, while `V` is linear over its domain. This may explain the difference in contributed variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The *grama* language\n",
    "\n",
    "---\n",
    "\n",
    "As a language, *grama* has both *objects* and *verbs*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objects\n",
    "\n",
    "---\n",
    "\n",
    "*grama* as a language considers two categories of objects:\n",
    "\n",
    "- **data**: observations on various quantities, implemented by the Python package `Pandas`\n",
    "- **models**: a function and complete description of its inputs, implemented by `py_grama`\n",
    "\n",
    "Since data is already well-handled by Pandas, `py_grama` focuses on providing tools to handle models. A `py_grama` model has **functions** and **inputs**:  The method `printpretty()` gives a quick summary of the model's inputs and function outputs. Model inputs are organized into:\n",
    "\n",
    "|            | Deterministic                            | Random     |\n",
    "| ---------- | ---------------------------------------- | ---------- |\n",
    "| Variables  | `model.var_det`                          | `model.var_rand` |\n",
    "| Parameters | `model.density.marginals[i].d_param`     | (Future*)  |\n",
    "\n",
    "- **Variables** are inputs to the model's functions\n",
    "  + **Deterministic** variables are chosen by the user; the model above has `w, t`\n",
    "  + **Random** variables are not controlled; the model above has `H, V, E, Y`\n",
    "- **Parameters** define random variables\n",
    "  + **Deterministic** parameters are currently implemented; these are listed under `var_rand` with their associated random variable\n",
    "  + **Random** parameters* are not yet implemented\n",
    "\n",
    "The `outputs` section lists the various model outputs. The model above has `c_area, g_stress, g_displacement`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbs\n",
    "\n",
    "---\n",
    "\n",
    "*grama* as a language has four categories of verbs, which are sorted based on the objects they take and return:\n",
    "\n",
    "| Category  | Stem (Short) | In    | Out   |\n",
    "| --------- | ------------ | ----- | ----- |\n",
    "| Evaluate  | eval_ (ev_)  | Model | Data  |\n",
    "| Fit       | fit_  (ft_)  | Data  | Model |\n",
    "| Transform | tran_ (tf_)  | Data  | Data  |\n",
    "| Compose   | compo (cp_)  | Model | Model |\n",
    "\n",
    "`py_grama` function names start with a stem, then continue with the specific function name. Both long and short forms exist to distinguish between vanilla functions and *pipe-enabled* versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional programming (Pipes)\n",
    "\n",
    "---\n",
    "\n",
    "`py_grama` provides tools to use functional programming patterns. Short-stem versions of `py_grama` functions are *pipe-enabled*, meaning they can be used in functional programming form with the pipe operator `>>`. These pipe-enabled functions are simply aliases for the base functions, as demonstrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = gr.eval_nominal(model_beam, df_det=\"nom\")\n",
    "df_functional = model_beam >> gr.ev_nominal(df_det=\"nom\")\n",
    "\n",
    "df_base.equals(df_functional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functional patterns enable chaining multiple commands, as demonstrated in the Sobol' index code above. In nested form using base functions, this would be:\n",
    "\n",
    "```python\n",
    "df_sobol = gr.tran_sobol(gr.eval_hybrid(model_beam, n_samples=1e3, df_det=\"nom\", seed=101))\n",
    "```\n",
    "\n",
    "From the code above, it is difficult to see that we first consider `model_beam`, perform a hybrid-point evaluation, then use those data to estimate Sobol' indices. With more chained functions, this only becomes more difficult. In functional form, the order of operations is given by the code order:\n",
    "\n",
    "```python\n",
    "df_sobol = \\\n",
    "    model_beam >> \\\n",
    "    gr.ev_hybrid(n_samples=1e3, df_det=\"nom\", seed=101) >> \\\n",
    "    gr.tf_sobol()\n",
    "```\n",
    "\n",
    "The other advantage of using functional patterns with the `>>` pipe is that `py_grama` functions can then be chained with functions from `dfply`, which provides pipe-enabled calls to `Pandas` functions. This allows us to combine *data science* tools with *model analysis tools*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Science Integration\n",
    "\n",
    "---\n",
    "\n",
    "To demonstrate the data science integration of `py_grama`, let's take apart the *sinew plot* analysis carried out above. The sinew design was generated by the call\n",
    "\n",
    "```python\n",
    "df_beam_sweeps = \\\n",
    "    model_beam >> \\\n",
    "    gr.ev_sinews(n_density=50, n_sweeps=10, df_det=\"nom\")\n",
    "```\n",
    "\n",
    "which produced the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beam_sweeps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data were then manipulated with a sequence of calls to `dfply`:\n",
    "\n",
    "```python\n",
    "df_beam_sweeps >> \\\n",
    "    gather(\"input\", \"x\", model_beam.var_rand) >> \\\n",
    "    gather(\"output\", \"y\", model_beam.outputs) >> \\\n",
    "    mask(X.sweep_var == X.input)\n",
    "```\n",
    "\n",
    "Step by step, we first *gather* the variables `model_beam.var_rand = [H,V,E,Y]` into key columns `input` and value columns `x`. This \"gathers\" the data from a wide table into a taller form, shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beam_sweeps >> \\\n",
    "    gather(\"input\", \"x\", model_beam.var_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the columns `[H,V,E,Y]` are now gone, replaced with codes `input` for the variable names and values stored in the `x` column.\n",
    "\n",
    "We carry out a similar operation on `model_beam.outputs = [c_area, g_stress, g_displacement]`, which yields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beam_sweeps >> \\\n",
    "    gather(\"input\", \"x\", model_beam.var_rand) >> \\\n",
    "    gather(\"output\", \"y\", model_beam.outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a gathered set of inputs `x` and outputs `y`, which we could plot all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    data=df_beam_sweeps >> \\\n",
    "        gather(\"input\", \"x\", model_beam.var_rand) >> \\\n",
    "        gather(\"output\", \"y\", model_beam.outputs),\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    hue=\"sweep_ind\",\n",
    "    kind=\"line\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is an utterly confusing plot; we're showing multiple quantities with different units on the same axes. To solve this issue but still show as much of the data as possible, we can use *facets* (sometimes called [small multiples](https://en.wikipedia.org/wiki/Small_multiple)). This will break the plot into a grid of smaller plots, one for each input/output pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    data=df_beam_sweeps >> \\\n",
    "        gather(\"input\", \"x\", model_beam.var_rand) >> \\\n",
    "        gather(\"output\", \"y\", model_beam.outputs),\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    hue=\"sweep_ind\",\n",
    "    col=\"input\",\n",
    "    row=\"output\",\n",
    "    kind=\"line\",\n",
    "    facet_kws=dict(sharex=False, sharey=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have used the `facet_kws` optional argument to allow each subplot to scale its own axis to fit its subset of the data.\n",
    "\n",
    "This is close to the plot we saw above, but with a number of \"spikes\". This is not a lack of smoothness in the response, but rather a consequence of samples \"out-of-plane\" being visualized in each facet. To solve this, we add one last `mask` call to the chain of data commands, \"masking out\" the out-of-plane cases. The following is the final version of the plot, showed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    data=df_beam_sweeps >> \\\n",
    "        gather(\"input\", \"x\", model_beam.var_rand) >> \\\n",
    "        gather(\"output\", \"y\", model_beam.outputs) >> \\\n",
    "        mask(X.sweep_var == X.input),\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    hue=\"sweep_ind\",\n",
    "    col=\"input\",\n",
    "    row=\"output\",\n",
    "    kind=\"line\",\n",
    "    facet_kws=dict(sharex=False, sharey=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a combination of `py_grama` and existing data science tools, one can easily carry out *exploratory model analysis*, like the analysis shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: It appears that the original [dfply](https://github.com/kieferk/dfply) may no longer be updated; I have forked [my own version](https://github.com/zdelrosario/dfply) which I will maintain to support `py_grama`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
